{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":708.779992,"end_time":"2022-07-15T11:49:20.809672","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-07-15T11:37:32.029680","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Automatic Speech Recognition with Transformer\n","metadata":{"papermill":{"duration":0.005394,"end_time":"2022-07-15T11:37:40.944768","exception":false,"start_time":"2022-07-15T11:37:40.939374","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":" i will use the LJSpeech dataset from the\n[LibriVox](https://librivox.org/) project. It consists of short\naudio clips of a single speaker reading passages from 7 non-fiction books.\n","metadata":{"papermill":{"duration":0.004148,"end_time":"2022-07-15T11:37:40.953499","exception":false,"start_time":"2022-07-15T11:37:40.949351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nimport os\nimport random\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:40.964444Z","iopub.status.busy":"2022-07-15T11:37:40.963703Z","iopub.status.idle":"2022-07-15T11:37:47.162173Z","shell.execute_reply":"2022-07-15T11:37:47.161215Z"},"papermill":{"duration":6.206889,"end_time":"2022-07-15T11:37:47.164713","exception":false,"start_time":"2022-07-15T11:37:40.957824","status":"completed"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Define the Transformer Input Layer\n\nWhen processing past target tokens for the decoder, we compute the sum of\nposition embeddings and token embeddings.\n\nWhen processing audio features, we apply convolutional layers to downsample\nthem (via convolution strides) and process local relationships.","metadata":{"papermill":{"duration":0.004769,"end_time":"2022-07-15T11:37:47.174287","exception":false,"start_time":"2022-07-15T11:37:47.169518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass TokenEmbedding(layers.Layer):\n    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n        super().__init__()\n        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        x = self.emb(x)\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        return x + positions\n\n\nclass SpeechFeatureEmbedding(layers.Layer):\n    def __init__(self, num_hid=64, maxlen=100):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.conv2 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.conv3 = tf.keras.layers.Conv1D(\n            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n        )\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return self.conv3(x)\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:47.184634Z","iopub.status.busy":"2022-07-15T11:37:47.184134Z","iopub.status.idle":"2022-07-15T11:37:47.195241Z","shell.execute_reply":"2022-07-15T11:37:47.194413Z"},"papermill":{"duration":0.018251,"end_time":"2022-07-15T11:37:47.197062","exception":false,"start_time":"2022-07-15T11:37:47.178811","status":"completed"},"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Encoder Layer","metadata":{"papermill":{"duration":0.004207,"end_time":"2022-07-15T11:37:47.205626","exception":false,"start_time":"2022-07-15T11:37:47.201419","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:47.215457Z","iopub.status.busy":"2022-07-15T11:37:47.215209Z","iopub.status.idle":"2022-07-15T11:37:47.223212Z","shell.execute_reply":"2022-07-15T11:37:47.222320Z"},"papermill":{"duration":0.015037,"end_time":"2022-07-15T11:37:47.225073","exception":false,"start_time":"2022-07-15T11:37:47.210036","status":"completed"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Decoder Layer","metadata":{"papermill":{"duration":0.004255,"end_time":"2022-07-15T11:37:47.233680","exception":false,"start_time":"2022-07-15T11:37:47.229425","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n        super().__init__()\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.self_att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.self_dropout = layers.Dropout(0.5)\n        self.enc_dropout = layers.Dropout(0.1)\n        self.ffn_dropout = layers.Dropout(0.1)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(feed_forward_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n\n    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n        \"\"\"Masks the upper half of the dot product matrix in self attention.\n\n        This prevents flow of information from future tokens to current token.\n        1's in the lower triangle, counting from the lower right corner.\n        \"\"\"\n        i = tf.range(n_dest)[:, None]\n        j = tf.range(n_src)\n        m = i >= j - n_src + n_dest\n        mask = tf.cast(m, dtype)\n        mask = tf.reshape(mask, [1, n_dest, n_src])\n        mult = tf.concat(\n            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n        )\n        return tf.tile(mask, mult)\n\n    def call(self, enc_out, target):\n        input_shape = tf.shape(target)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n        target_att = self.self_att(target, target, attention_mask=causal_mask)\n        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n        enc_out = self.enc_att(target_norm, enc_out)\n        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n        ffn_out = self.ffn(enc_out_norm)\n        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n        return ffn_out_norm\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:47.243808Z","iopub.status.busy":"2022-07-15T11:37:47.243563Z","iopub.status.idle":"2022-07-15T11:37:47.256672Z","shell.execute_reply":"2022-07-15T11:37:47.255730Z"},"papermill":{"duration":0.02039,"end_time":"2022-07-15T11:37:47.258527","exception":false,"start_time":"2022-07-15T11:37:47.238137","status":"completed"},"tags":[]},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Complete the Transformer model\n\n model takes audio spectrograms as inputs and predicts a sequence of characters.\nDuring training, i give the decoder the target character sequence shifted to the left\nas input. During inference, the decoder uses its own past predictions to predict the\nnext token.","metadata":{"papermill":{"duration":0.00426,"end_time":"2022-07-15T11:37:47.267250","exception":false,"start_time":"2022-07-15T11:37:47.262990","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass Transformer(keras.Model):\n    def __init__(\n        self,\n        num_hid=64,\n        num_head=2,\n        num_feed_forward=128,\n        source_maxlen=100,\n        target_maxlen=100,\n        num_layers_enc=4,\n        num_layers_dec=1,\n        num_classes=10,\n    ):\n        super().__init__()\n        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n        self.num_layers_enc = num_layers_enc\n        self.num_layers_dec = num_layers_dec\n        self.target_maxlen = target_maxlen\n        self.num_classes = num_classes\n\n        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n        self.dec_input = TokenEmbedding(\n            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n        )\n\n        self.encoder = keras.Sequential(\n            [self.enc_input]\n            + [\n                TransformerEncoder(num_hid, num_head, num_feed_forward)\n                for _ in range(num_layers_enc)\n            ]\n        )\n\n        for i in range(num_layers_dec):\n            setattr(\n                self,\n                f\"dec_layer_{i}\",\n                TransformerDecoder(num_hid, num_head, num_feed_forward),\n            )\n\n        self.classifier = layers.Dense(num_classes)\n\n    def decode(self, enc_out, target):\n        y = self.dec_input(target)\n        for i in range(self.num_layers_dec):\n            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n        return y\n\n    def call(self, inputs):\n        source = inputs[0]\n        target = inputs[1]\n        x = self.encoder(source)\n        y = self.decode(x, target)\n        return self.classifier(y)\n\n    @property\n    def metrics(self):\n        return [self.loss_metric]\n\n    def train_step(self, batch):\n        \"\"\"Processes one batch inside model.fit().\"\"\"\n        source = batch[\"source\"]\n        target = batch[\"target\"]\n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        with tf.GradientTape() as tape:\n            preds = self([source, dec_input])\n            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def test_step(self, batch):\n        source = batch[\"source\"]\n        target = batch[\"target\"]\n        dec_input = target[:, :-1]\n        dec_target = target[:, 1:]\n        preds = self([source, dec_input])\n        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n        self.loss_metric.update_state(loss)\n        return {\"loss\": self.loss_metric.result()}\n\n    def generate(self, source, target_start_token_idx):\n        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n        bs = tf.shape(source)[0]\n        enc = self.encoder(source)\n        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n        dec_logits = []\n        for i in range(self.target_maxlen - 1):\n            dec_out = self.decode(enc, dec_input)\n            logits = self.classifier(dec_out)\n            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n            dec_logits.append(last_logit)\n            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n        return dec_input\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:47.277772Z","iopub.status.busy":"2022-07-15T11:37:47.277041Z","iopub.status.idle":"2022-07-15T11:37:47.299170Z","shell.execute_reply":"2022-07-15T11:37:47.298346Z"},"papermill":{"duration":0.029261,"end_time":"2022-07-15T11:37:47.301010","exception":false,"start_time":"2022-07-15T11:37:47.271749","status":"completed"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Download the dataset\n\nNote: This requires ~3.6 GB of disk space and\ntakes ~5 minutes for the extraction of files.","metadata":{"papermill":{"duration":0.004375,"end_time":"2022-07-15T11:37:47.310070","exception":false,"start_time":"2022-07-15T11:37:47.305695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.get_file(\n    os.path.join(os.getcwd(), \"data.tar.gz\"),\n    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n    extract=True,\n    archive_format=\"tar\",\n    cache_dir=\".\",\n)\n\n\nsaveto = \"./datasets/LJSpeech-1.1\"\nwavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n\nid_to_text = {}\nwith open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n    for line in f:\n        id = line.strip().split(\"|\")[0]\n        text = line.strip().split(\"|\")[2]\n        id_to_text[id] = text\n\n\ndef get_data(wavs, id_to_text, maxlen=50):\n    \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n    data = []\n    for w in wavs:\n        id = w.split(\"/\")[-1].split(\".\")[0]\n        if len(id_to_text[id]) < maxlen:\n            data.append({\"audio\": w, \"text\": id_to_text[id]})\n    return data\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:37:47.320860Z","iopub.status.busy":"2022-07-15T11:37:47.320076Z","iopub.status.idle":"2022-07-15T11:43:22.571163Z","shell.execute_reply":"2022-07-15T11:43:22.570038Z"},"papermill":{"duration":335.259842,"end_time":"2022-07-15T11:43:22.574490","exception":false,"start_time":"2022-07-15T11:37:47.314648","status":"completed"},"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n\n2748579840/2748572632 [==============================] - 62s 0us/step\n\n2748588032/2748572632 [==============================] - 62s 0us/step\n"}]},{"cell_type":"markdown","source":"## Preprocess the dataset","metadata":{"papermill":{"duration":0.106432,"end_time":"2022-07-15T11:43:22.789510","exception":false,"start_time":"2022-07-15T11:43:22.683078","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass VectorizeChar:\n    def __init__(self, max_len=50):\n        self.vocab = (\n            [\"-\", \"#\", \"<\", \">\"]\n            + [chr(i + 96) for i in range(1, 27)]\n            + [\" \", \".\", \",\", \"?\"]\n        )\n        self.max_len = max_len\n        self.char_to_idx = {}\n        for i, ch in enumerate(self.vocab):\n            self.char_to_idx[ch] = i\n\n    def __call__(self, text):\n        text = text.lower()\n        text = text[: self.max_len - 2]\n        text = \"<\" + text + \">\"\n        pad_len = self.max_len - len(text)\n        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n\n    def get_vocabulary(self):\n        return self.vocab\n\n\nmax_target_len = 200  # all transcripts in out data are < 200 characters\ndata = get_data(wavs, id_to_text, max_target_len)\nvectorizer = VectorizeChar(max_target_len)\nprint(\"vocab size\", len(vectorizer.get_vocabulary()))\n\n\ndef create_text_ds(data):\n    texts = [_[\"text\"] for _ in data]\n    text_ds = [vectorizer(t) for t in texts]\n    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n    return text_ds\n\n\ndef path_to_audio(path):\n    # spectrogram using stft\n    audio = tf.io.read_file(path)\n    audio, _ = tf.audio.decode_wav(audio, 1)\n    audio = tf.squeeze(audio, axis=-1)\n    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n    x = tf.math.pow(tf.abs(stfts), 0.5)\n    # normalisation\n    means = tf.math.reduce_mean(x, 1, keepdims=True)\n    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n    x = (x - means) / stddevs\n    audio_len = tf.shape(x)[0]\n    # padding to 10 seconds\n    pad_len = 2754\n    paddings = tf.constant([[0, pad_len], [0, 0]])\n    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n    return x\n\n\ndef create_audio_ds(data):\n    flist = [_[\"audio\"] for _ in data]\n    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n    audio_ds = audio_ds.map(\n        path_to_audio, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    return audio_ds\n\n\ndef create_tf_dataset(data, bs=4):\n    audio_ds = create_audio_ds(data)\n    text_ds = create_text_ds(data)\n    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n    ds = ds.batch(bs)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\nsplit = int(len(data) * 0.99)\ntrain_data = data[:split]\ntest_data = data[split:]\nds = create_tf_dataset(train_data, bs=64)\nval_ds = create_tf_dataset(test_data, bs=4)","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:43:22.943240Z","iopub.status.busy":"2022-07-15T11:43:22.942895Z","iopub.status.idle":"2022-07-15T11:43:34.853495Z","shell.execute_reply":"2022-07-15T11:43:34.852504Z"},"papermill":{"duration":11.985433,"end_time":"2022-07-15T11:43:34.858095","exception":false,"start_time":"2022-07-15T11:43:22.872662","status":"completed"},"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"vocab size 34\n"},{"name":"stderr","output_type":"stream","text":"2022-07-15 11:43:23.092955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:23.207080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:23.207918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:23.212244: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n2022-07-15 11:43:23.212581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:23.213594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:23.214588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:25.789938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:25.790770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:25.791457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n2022-07-15 11:43:25.793142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"}]},{"cell_type":"markdown","source":"## Callbacks to display predictions","metadata":{"papermill":{"duration":0.130899,"end_time":"2022-07-15T11:43:35.111137","exception":false,"start_time":"2022-07-15T11:43:34.980238","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass DisplayOutputs(keras.callbacks.Callback):\n    def __init__(\n        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n    ):\n        \"\"\"Displays a batch of outputs after every epoch\n\n        Args:\n            batch: A test batch containing the keys \"source\" and \"target\"\n            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n            target_start_token_idx: A start token index in the target vocabulary\n            target_end_token_idx: An end token index in the target vocabulary\n        \"\"\"\n        self.batch = batch\n        self.target_start_token_idx = target_start_token_idx\n        self.target_end_token_idx = target_end_token_idx\n        self.idx_to_char = idx_to_token\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 5 != 0:\n            return\n        source = self.batch[\"source\"]\n        target = self.batch[\"target\"].numpy()\n        bs = tf.shape(source)[0]\n        preds = self.model.generate(source, self.target_start_token_idx)\n        preds = preds.numpy()\n        for i in range(bs):\n            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n            prediction = \"\"\n            for idx in preds[i, :]:\n                prediction += self.idx_to_char[idx]\n                if idx == self.target_end_token_idx:\n                    break\n            print(f\"target:     {target_text.replace('-','')}\")\n            print(f\"prediction: {prediction}\\n\")\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:43:35.345429Z","iopub.status.busy":"2022-07-15T11:43:35.345028Z","iopub.status.idle":"2022-07-15T11:43:35.359896Z","shell.execute_reply":"2022-07-15T11:43:35.359060Z"},"papermill":{"duration":0.127994,"end_time":"2022-07-15T11:43:35.362451","exception":false,"start_time":"2022-07-15T11:43:35.234457","status":"completed"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Learning rate schedule","metadata":{"papermill":{"duration":0.070289,"end_time":"2022-07-15T11:43:35.543107","exception":false,"start_time":"2022-07-15T11:43:35.472818","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nclass CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(\n        self,\n        init_lr=0.00001,\n        lr_after_warmup=0.001,\n        final_lr=0.00001,\n        warmup_epochs=15,\n        decay_epochs=85,\n        steps_per_epoch=203,\n    ):\n        super().__init__()\n        self.init_lr = init_lr\n        self.lr_after_warmup = lr_after_warmup\n        self.final_lr = final_lr\n        self.warmup_epochs = warmup_epochs\n        self.decay_epochs = decay_epochs\n        self.steps_per_epoch = steps_per_epoch\n\n    def calculate_lr(self, epoch):\n        \"\"\" linear warm up - linear decay \"\"\"\n        warmup_lr = (\n            self.init_lr\n            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n        )\n        decay_lr = tf.math.maximum(\n            self.final_lr,\n            self.lr_after_warmup\n            - (epoch - self.warmup_epochs)\n            * (self.lr_after_warmup - self.final_lr)\n            / (self.decay_epochs),\n        )\n        return tf.math.minimum(warmup_lr, decay_lr)\n\n    def __call__(self, step):\n        epoch = step // self.steps_per_epoch\n        return self.calculate_lr(epoch)\n","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:43:35.677610Z","iopub.status.busy":"2022-07-15T11:43:35.676777Z","iopub.status.idle":"2022-07-15T11:43:35.686861Z","shell.execute_reply":"2022-07-15T11:43:35.685860Z"},"papermill":{"duration":0.080617,"end_time":"2022-07-15T11:43:35.688823","exception":false,"start_time":"2022-07-15T11:43:35.608206","status":"completed"},"tags":[]},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Create & train the end-to-end model","metadata":{"papermill":{"duration":0.064745,"end_time":"2022-07-15T11:43:35.819214","exception":false,"start_time":"2022-07-15T11:43:35.754469","status":"completed"},"tags":[]}},{"cell_type":"code","source":"batch = next(iter(val_ds))\n\n# The vocabulary to convert predicted indices into characters\nidx_to_char = vectorizer.get_vocabulary()\ndisplay_cb = DisplayOutputs(\n    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n)  # set the arguments as per vocabulary index for '<' and '>'\n\nmodel = Transformer(\n    num_hid=200,\n    num_head=2,\n    num_feed_forward=400,\n    target_maxlen=max_target_len,\n    num_layers_enc=4,\n    num_layers_dec=1,\n    num_classes=34,\n)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(\n    from_logits=True, label_smoothing=0.1,\n)\n\nlearning_rate = CustomSchedule(\n    init_lr=0.00001,\n    lr_after_warmup=0.001,\n    final_lr=0.00001,\n    warmup_epochs=15,\n    decay_epochs=85,\n    steps_per_epoch=len(ds),\n)\noptimizer = keras.optimizers.Adam(learning_rate)\nmodel.compile(optimizer=optimizer, loss=loss_fn)\n\nhistory = model.fit(ds, validation_data=val_ds, callbacks=[display_cb], epochs=1)","metadata":{"execution":{"iopub.execute_input":"2022-07-15T11:43:35.952736Z","iopub.status.busy":"2022-07-15T11:43:35.951695Z","iopub.status.idle":"2022-07-15T11:49:16.974856Z","shell.execute_reply":"2022-07-15T11:49:16.973724Z"},"papermill":{"duration":341.168361,"end_time":"2022-07-15T11:49:17.053110","exception":false,"start_time":"2022-07-15T11:43:35.884749","status":"completed"},"tags":[]},"execution_count":10,"outputs":[{"name":"stderr","output_type":"stream","text":"2022-07-15 11:43:36.009296: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n2022-07-15 11:43:43.442441: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 90948096 exceeds 10% of free system memory.\n\n2022-07-15 11:43:45.459275: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 90948096 exceeds 10% of free system memory.\n\n2022-07-15 11:43:45.980290: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"},{"name":"stdout","output_type":"stream","text":"  2/203 [..............................] - ETA: 55s - loss: 2.0566  "},{"name":"stderr","output_type":"stream","text":"2022-07-15 11:43:53.165279: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 90948096 exceeds 10% of free system memory.\n"},{"name":"stdout","output_type":"stream","text":"  3/203 [..............................] - ETA: 3:05 - loss: 2.0692"},{"name":"stderr","output_type":"stream","text":"2022-07-15 11:43:54.653524: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 90948096 exceeds 10% of free system memory.\n"},{"name":"stdout","output_type":"stream","text":"  4/203 [..............................] - ETA: 3:39 - loss: 2.0781"},{"name":"stderr","output_type":"stream","text":"2022-07-15 11:43:56.474299: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 90948096 exceeds 10% of free system memory.\n"},{"name":"stdout","output_type":"stream","text":"203/203 [==============================] - 338s 2s/step - loss: 1.7319 - val_loss: 1.5966\n\ntarget:     <and was arrested on board ship just as he was about to leave the country, had a little spare cash, which he devoted entirely to the purchase of extra food.>\n\nprediction: <the t the the e o anre t t the t one  t t os o ce an the  w s the t t t oa o is t  od oaint th s os t t inenthe te t the  ian t on there th t the o o o ine o inre onen s t o the sss ilere th the  s i\n\n\n\ntarget:     <there was a smile on his face when it was last seen, and just as the terrible white cap was drawn over it.>\n\nprediction: <the t the the e o anre t t the t one  t t os o ce an the  w s the t t t oa o is t  od oaint th s os t t inenthe te t the  ian t on there os t the tho o ine o inre onen s t o the sss ilere th the  s i\n\n\n\ntarget:     <this man i saw previously was aiming for his last shot. as it appeared to me he was standing up and resting against the left window sill, end quote.>\n\nprediction: <the t the the e o anre t t the t one  t t os o ce an the  w s the t t t oa o is t  od oaint th s os t t inenthe te t the  ian t on there os t the t o o ine o inre onen s t o the sss ilere th the  s i\n\n\n\ntarget:     <the sassanian kings of persia were fond of hunting, and babylon, then overgrown with trees, was their game preserve.>\n\nprediction: <the t the the e o anre t t the t one  t t os o ce an the  w s the t t t oa o is t  od oaint th s os t t inenthe te t the  ian t on there os t the t o o ine o inre onen s t o the sss ilere th the  s i\n\n\n"}]},{"cell_type":"markdown","source":"In practice, you should train for around 100 epochs or more.\n\nSome of the predicted text at or around epoch 35 may look as follows:\n```\ntarget:     <as they sat in the car, frazier asked oswald where his lunch was>\nprediction: <as they sat in the car frazier his lunch ware mis lunch was>\n\ntarget:     <under the entry for may one, nineteen sixty,>\nprediction: <under the introus for may monee, nin the sixty,>\n```","metadata":{"papermill":{"duration":0.075873,"end_time":"2022-07-15T11:49:17.206343","exception":false,"start_time":"2022-07-15T11:49:17.130470","status":"completed"},"tags":[]}}]}